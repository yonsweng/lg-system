{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Libraries and Initial Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "CUDA         = 1\n",
    "RANDOM_STATE = 2021"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{CUDA}\"\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np   .random.seed(RANDOM_STATE)"
   ]
  },
  {
   "source": [
    "# Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data'\n",
    "train_err     = pd.read_csv(os.path.join(PATH,     'train_err_data.csv'))\n",
    "train_problem = pd.read_csv(os.path.join(PATH, 'train_problem_data.csv'))\n",
    "test_err      = pd.read_csv(os.path.join(PATH,      'test_err_data.csv'))"
   ]
  },
  {
   "source": [
    "# Data Prepration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(err, onehot_encoder, min_id, max_id):\n",
    "    user_ids = list(err['user_id'])\n",
    "\n",
    "    # one-hot encode errtype\n",
    "    onehot_errtypes = onehot_encoder.transform(err['errtype'])\n",
    "    features = list(onehot_errtypes)  # [np.array([0, ...]), ...]\n",
    "\n",
    "    x = [[] for _ in range(max_id - min_id + 1)]\n",
    "\n",
    "    # store one-hot errtype into x\n",
    "    for user_id, feature in zip(user_ids, features):\n",
    "        i = user_id - min_id\n",
    "        x[i].append(feature)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.array(x[i])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_id, train_max_id = train_err['user_id'].min(), train_err['user_id'].max()\n",
    "test_min_id ,  test_max_id =  test_err['user_id'].min(),  test_err['user_id'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LabelBinarizer()"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "onehot_encoder = LabelBinarizer()\n",
    "onehot_encoder.fit(train_err['errtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of np.array(seq_len, #features)\n",
    "train_x = preprocess(train_err, onehot_encoder, train_min_id, train_max_id)\n",
    "test_x  = preprocess( test_err, onehot_encoder,  test_min_id,  test_max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "222186\n396478\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for arr in train_x:\n",
    "    max_len = max(max_len, len(arr))\n",
    "print(max_len)\n",
    "for arr in test_x:\n",
    "    max_len = max(max_len, len(arr))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_y\n",
    "problem_user_ids = sorted(train_problem['user_id'].unique())\n",
    "train_y = [0] * (train_max_id - train_min_id + 1)\n",
    "for user_id in problem_user_ids:\n",
    "    i = user_id - train_min_id\n",
    "    train_y[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "input_size = train_x[0].shape[-1]\n",
    "input_size"
   ]
  },
  {
   "source": [
    "# Define a Model Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, stride, dropout=0.5):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_size , hidden_size, kernel_size, stride)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, stride)\n",
    "        self.pool = nn.MaxPool1d(8)\n",
    "        self.small_dropout = nn.Dropout(0.15)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 126, hidden_size * 16),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (batch_size, input_size, seq_len)\n",
    "        x = self.small_dropout(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # x=(batch_size, hidden_size, *)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # x=(batch_size, hidden_size, *)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Dropout-1           [-1, 41, 396478]               0\n            Conv1d-2            [-1, 64, 56638]          39,424\n           Dropout-3            [-1, 64, 56638]               0\n         MaxPool1d-4             [-1, 64, 7079]               0\n            Conv1d-5             [-1, 64, 1010]          61,504\n           Dropout-6             [-1, 64, 1010]               0\n         MaxPool1d-7              [-1, 64, 126]               0\n            Linear-8                 [-1, 1024]       8,258,560\n           Dropout-9                 [-1, 1024]               0\n             ReLU-10                 [-1, 1024]               0\n           Linear-11                    [-1, 1]           1,025\n          Sigmoid-12                    [-1, 1]               0\n================================================================\nTotal params: 8,360,513\nTrainable params: 8,360,513\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 62.01\nForward/backward pass size (MB): 183.86\nParams size (MB): 31.89\nEstimated Total Size (MB): 277.76\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net(41, 64, 15, 7).to(device)\n",
    "from torchsummary import summary\n",
    "summary(net, (41, 396478))"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# hyperparameters for learning\n",
    "LR = 1e-3\n",
    "HIDDEN_SIZE = 256\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "best_aucs = [0.] * k_fold.get_n_splits()\n",
    "\n",
    "for i_fold, (train_idx, valid_idx) in enumerate(k_fold.split(train_x)):\n",
    "    print(f'Fold {i_fold} started!')\n",
    "\n",
    "    net = Net(input_size, HIDDEN_SIZE).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    stop_cnt = 0\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        print(f'Epoch {epoch} ', end='\\t')\n",
    "\n",
    "        # training\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        for idx in tqdm(train_idx):\n",
    "            inputs = torch.tensor(train_x[idx],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "            labels = torch.tensor([train_y[idx]],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(outputs)\n",
    "            saved_outputs.append(outputs        .item())\n",
    "            saved_labels .append((labels >= 0.5).item())\n",
    "\n",
    "        running_loss /= len(train_idx)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'train loss={running_loss:.3f} \\t'\n",
    "              f'train auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        # validation\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        for idx in tqdm(valid_idx):\n",
    "            inputs = torch.tensor(train_x[idx],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "            labels = torch.tensor([train_y[idx]],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * len(outputs)\n",
    "            saved_outputs.append(outputs        .item())\n",
    "            saved_labels .append((labels >= 0.5).item())\n",
    "\n",
    "        running_loss /= len(valid_idx)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'valid loss={running_loss:.3f} \\t'\n",
    "              f'valid auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        if auc > best_aucs[i_fold]:\n",
    "            best_aucs[i_fold] = auc\n",
    "            torch.save(net.state_dict(), f'../models/{i_fold}.pt')\n",
    "            print(f'model saved!', end='')\n",
    "            stop_cnt = 0\n",
    "        else:\n",
    "            stop_cnt += 1\n",
    "            if stop_cnt > PATIENCE:\n",
    "                print()\n",
    "                break\n",
    "        print()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 96,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/12000 [00:00<17:30, 11.42it/s]Fold 0 started!\n",
      "100%|██████████| 12000/12000 [09:07<00:00, 21.92it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-088afddada5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msaved_labels\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ]
  },
  {
   "source": [
    "# Make a Submission File"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "testset = Dataset(testdata)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "net = Net(INPUT_NUM)\n",
    "net.to(device) \n",
    "\n",
    "votes = np.zeros(len(testloader.dataset))\n",
    "\n",
    "for i in range(k_fold.get_n_splits()):\n",
    "    net.load_state_dict(torch.load(f'../models/{i}.pt'))\n",
    "\n",
    "    saved_outputs = []\n",
    "\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        saved_outputs += outputs.squeeze().tolist()\n",
    "\n",
    "    votes += np.array(saved_outputs)\n",
    "\n",
    "votes = votes / k_fold.get_n_splits()\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))\n",
    "sample_submission['problem'] = votes\n",
    "sample_submission.to_csv('../submission.csv', index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Print the AUC Score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc = sum(best_aucs) / len(best_aucs)\n",
    "cv_auc"
   ]
  }
 ]
}