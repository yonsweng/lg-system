{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Libraries and Initial Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "CUDA         = 1\n",
    "RANDOM_STATE = 2021"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np   .random.seed(RANDOM_STATE)\n",
    "random      .seed(RANDOM_STATE)"
   ]
  },
  {
   "source": [
    "# Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data'\n",
    "train_err     = pd.read_csv(os.path.join(PATH,     'train_err_data.csv'))\n",
    "train_problem = pd.read_csv(os.path.join(PATH, 'train_problem_data.csv'))\n",
    "test_err      = pd.read_csv(os.path.join(PATH,      'test_err_data.csv'))"
   ]
  },
  {
   "source": [
    "# Data Prepration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(err, onehot_encoder, min_id, max_id):\n",
    "    user_ids = list(err['user_id'])\n",
    "\n",
    "    # one-hot encode errtype\n",
    "    onehot_errtypes = onehot_encoder.transform(err['errtype'])\n",
    "    features = list(onehot_errtypes)  # [np.array([0, ...]), ...]\n",
    "\n",
    "    x = [[] for _ in range(max_id - min_id + 1)]\n",
    "\n",
    "    # store one-hot errtype into x\n",
    "    for user_id, feature in zip(user_ids, features):\n",
    "        i = user_id - min_id\n",
    "        x[i].append(feature)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.array(x[i])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_id, train_max_id = train_err['user_id'].min(), train_err['user_id'].max()\n",
    "test_min_id ,  test_max_id =  test_err['user_id'].min(),  test_err['user_id'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = LabelBinarizer()\n",
    "onehot_encoder.fit(train_err['errtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of np.array(seq_len, #features)\n",
    "train_x = preprocess(train_err, onehot_encoder, train_min_id, train_max_id)\n",
    "test_x  = preprocess( test_err, onehot_encoder,  test_min_id,  test_max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_y\n",
    "problem_user_ids = sorted(train_problem['user_id'].unique())\n",
    "train_y = [0] * (train_max_id - train_min_id + 1)\n",
    "for user_id in problem_user_ids:\n",
    "    i = user_id - train_min_id\n",
    "    train_y[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_x[0].shape[-1]\n",
    "input_size"
   ]
  },
  {
   "source": [
    "# Define a Model Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, dropout=0)\n",
    "        self.dense = nn.Sequential(\n",
    "            # nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.Dropout(),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x, _ = self.rnn(x)  # (seq_len, batch_size, hidden_size)\n",
    "        x = torch.max(x, dim=0).values  # (batch_size, hidden_size)\n",
    "        x = self.dense(x)  # (batch_size, 1)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# hyperparameters for learning\n",
    "LR = 1e-4\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 1024\n",
    "MAX_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "best_aucs = [0.] * k_fold.get_n_splits()\n",
    "\n",
    "for i_fold, (train_idx, valid_idx) in enumerate(k_fold.split(train_x)):\n",
    "    print(f'Fold {i_fold} started!')\n",
    "\n",
    "    net = Net(input_size, HIDDEN_SIZE).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    stop_cnt = 0\n",
    "\n",
    "    batch_loss = 0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        print(f'Epoch {epoch} ', end='\\t')\n",
    "\n",
    "        # training\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        random.shuffle(train_idx)\n",
    "\n",
    "        for idx in tqdm(train_idx):\n",
    "            inputs = torch.tensor(train_x[idx],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "            labels = torch.tensor([train_y[idx]],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_loss += loss\n",
    "            batch_cnt += 1\n",
    "            if batch_cnt == BATCH_SIZE:\n",
    "                batch_loss /= batch_cnt\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss = 0\n",
    "                batch_cnt = 0\n",
    "\n",
    "            running_loss += loss.item() * len(outputs)\n",
    "            saved_outputs.append(outputs        .item())\n",
    "            saved_labels .append((labels >= 0.5).item())\n",
    "\n",
    "        running_loss /= len(train_idx)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'train loss={running_loss:.3f} \\t'\n",
    "              f'train auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        # validation\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        for idx in tqdm(valid_idx):\n",
    "            inputs = torch.tensor(train_x[idx],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "            labels = torch.tensor([train_y[idx]],\n",
    "                dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * len(outputs)\n",
    "            saved_outputs.append(outputs        .item())\n",
    "            saved_labels .append((labels >= 0.5).item())\n",
    "\n",
    "        running_loss /= len(valid_idx)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'valid loss={running_loss:.3f} \\t'\n",
    "              f'valid auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        if auc > best_aucs[i_fold]:\n",
    "            best_aucs[i_fold] = auc\n",
    "            torch.save(net.state_dict(), f'../models/{i_fold}.pt')\n",
    "            print(f'model saved!', end='')\n",
    "            stop_cnt = 0\n",
    "        else:\n",
    "            stop_cnt += 1\n",
    "            if stop_cnt > PATIENCE:\n",
    "                print()\n",
    "                break\n",
    "        print()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Make a Submission File"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "testset = Dataset(testdata)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "net = Net(INPUT_NUM)\n",
    "net.to(device) \n",
    "\n",
    "votes = np.zeros(len(testloader.dataset))\n",
    "\n",
    "for i in range(k_fold.get_n_splits()):\n",
    "    net.load_state_dict(torch.load(f'../models/{i}.pt'))\n",
    "\n",
    "    saved_outputs = []\n",
    "\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        saved_outputs += outputs.squeeze().tolist()\n",
    "\n",
    "    votes += np.array(saved_outputs)\n",
    "\n",
    "votes = votes / k_fold.get_n_splits()\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))\n",
    "sample_submission['problem'] = votes\n",
    "sample_submission.to_csv('../submission.csv', index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Print the AUC Score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc = sum(best_aucs) / len(best_aucs)\n",
    "cv_auc"
   ]
  }
 ]
}