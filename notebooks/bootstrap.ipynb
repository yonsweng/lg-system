{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Libraries and Initial Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "CUDA = 1\n",
    "RANDOM_STATE = 2021\n",
    "device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data'\n",
    "train_err = pd.read_csv(os.path.join(PATH, 'train_err_data.csv'))\n",
    "train_problem = pd.read_csv(os.path.join(PATH, 'train_problem_data.csv'))\n",
    "test_err = pd.read_csv(os.path.join(PATH, 'test_err_data.csv'))"
   ]
  },
  {
   "source": [
    "# Define Dataset and Model Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = torch.Tensor(self.data[index, :-1])\n",
    "        label = torch.Tensor(self.data[index, -1:])\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_num):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(input_num, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "# Data Prepration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(err, problem, users, scaler=None):\n",
    "    global TOTAL_ERR_NUM, INPUT_NUM, used_code_dict\n",
    "\n",
    "    data = np.zeros((len(users), INPUT_NUM + 1))\n",
    "    user2idx = {user: idx for idx, user in enumerate(users)}\n",
    "\n",
    "    # errtype features\n",
    "    user_err = err[['user_id', 'errtype']].values\n",
    "    for user, errtype in tqdm(user_err):\n",
    "        idx = user2idx[user]\n",
    "        data[idx, errtype] += 1\n",
    "        data[idx, 0] += 1  # total_errs\n",
    "\n",
    "    # errcode features\n",
    "    code_start_index = 1 + TOTAL_ERR_NUM\n",
    "    user_id = err['user_id'].values\n",
    "    errcode = err['errcode'].values\n",
    "    for user, errcode in tqdm(zip(user_id, errcode)):\n",
    "        if errcode in used_code_dict:\n",
    "            col = code_start_index + used_code_dict[errcode]\n",
    "            data[user2idx[user], col] += 1\n",
    "\n",
    "    # model_changed feature\n",
    "    model_changed_col = 1 + TOTAL_ERR_NUM + len(used_code_dict)\n",
    "    model_nm = err['model_nm'].values\n",
    "    prev_user  = None\n",
    "    prev_model = None\n",
    "    for user, model in tqdm(zip(user_id, model_nm)):\n",
    "        if prev_user is None or prev_user != user:\n",
    "            prev_user  = user\n",
    "            prev_model = model\n",
    "        elif prev_model is None:\n",
    "            prev_model = model\n",
    "        elif prev_model != model:\n",
    "            prev_model = model\n",
    "            data[user2idx[user], model_changed_col] = 1.\n",
    "\n",
    "    # Standard-scale\n",
    "    scaler = None\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data[:, :-2])\n",
    "    data[:, :-2] = scaler.transform(data[:, :-2])\n",
    "\n",
    "    if problem is not None:  # if train data\n",
    "        true_targets = [user2idx[user] for user in problem['user_id']]\n",
    "        data[true_targets, -1] = 1.\n",
    "\n",
    "    return data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants for data preprocess\n",
    "TOTAL_ERR_NUM = 42\n",
    "USED_CODES = ['5', '6', 'V-21008', 'terminate by peer user', 'H-51042', '4',\n",
    "              'connection fail to establish', '82', '13', '14', '83', '99', '3', '88',\n",
    "              'connection timeout', '100', 'connectionterminated by local host', '91',\n",
    "              'UNKNOWN', '95', '87', '94', '78', '89', '90', '81', '86', 'active',\n",
    "              '85', '84', '2', 'NFANDROID2', 'S-61001', '1', '80', '79', 'B-A8002',\n",
    "              'standby', '8.0', '0', 'S-65002', 'Q-64002']\n",
    "INPUT_NUM = 1 + TOTAL_ERR_NUM + len(USED_CODES) + 1  # total_errs + ... + model_changed\n",
    "tv_users   = np.array(range(10000, 25000))\n",
    "test_users = np.array(range(30000, 44999))\n",
    "print('# of inputs:', INPUT_NUM)\n",
    "\n",
    "# get hashes such as used_code_dict\n",
    "used_code_dict = {code: i for i, code in enumerate(USED_CODES)}\n",
    "\n",
    "# get a scaler and make test data\n",
    "tvdata  , scaler = make_data(train_err, train_problem, tv_users)\n",
    "testdata, _      = make_data(test_err , None         , test_users, scaler)"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# hyperparameters for learning\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "best_aucs = [0.] * k_fold.get_n_splits()\n",
    "\n",
    "for i_fold, (train_idx, valid_idx) in enumerate(k_fold.split(tvdata)):\n",
    "    print(f'Fold {i_fold} started!')\n",
    "\n",
    "    traindata = tvdata[train_idx]\n",
    "    validdata = tvdata[valid_idx]\n",
    "\n",
    "    trainset = Dataset(traindata)\n",
    "    validset = Dataset(validdata)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = DataLoader(validset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    net = Net(INPUT_NUM).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    stop_cnt = 0\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        print(f'Epoch {epoch} ', end='\\t')\n",
    "\n",
    "        # Training\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss  += loss.item() * len(outputs)\n",
    "            saved_outputs += outputs.squeeze()        .tolist()\n",
    "            saved_labels  += (labels.squeeze() >= 0.5).tolist()\n",
    "\n",
    "        running_loss /= len(trainloader.dataset)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'train loss={running_loss:.3f} \\t'\n",
    "              f'train auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "        saved_outputs = []\n",
    "        saved_labels = []\n",
    "\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss  += loss.item() * len(outputs)\n",
    "            saved_outputs += outputs.squeeze()        .tolist()\n",
    "            saved_labels  += (labels.squeeze() >= 0.5).tolist()\n",
    "\n",
    "        running_loss /= len(validloader.dataset)\n",
    "        auc = roc_auc_score(saved_labels, saved_outputs)\n",
    "\n",
    "        print(f'valid loss={running_loss:.3f} \\t'\n",
    "              f'valid auc={ auc         :.3f} \\t', end='')\n",
    "\n",
    "        if auc > best_aucs[i_fold]:\n",
    "            best_aucs[i_fold] = auc\n",
    "            torch.save(net.state_dict(), f'../models/{i_fold}.pt')\n",
    "            print(f'model saved!', end='')\n",
    "            stop_cnt = 0\n",
    "        else:\n",
    "            stop_cnt += 1\n",
    "            if stop_cnt > PATIENCE:\n",
    "                print()\n",
    "                break\n",
    "        print()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Make a Submission File"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "testset = Dataset(testdata)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "net = Net(INPUT_NUM)\n",
    "net.to(device) \n",
    "\n",
    "votes = np.zeros(len(testloader.dataset))\n",
    "\n",
    "for i in range(k_fold.get_n_splits()):\n",
    "    net.load_state_dict(torch.load(f'../models/{i}.pt'))\n",
    "\n",
    "    saved_outputs = []\n",
    "\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        saved_outputs += outputs.squeeze().tolist()\n",
    "\n",
    "    votes += np.array(saved_outputs)\n",
    "\n",
    "votes = votes / k_fold.get_n_splits()\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))\n",
    "sample_submission['problem'] = votes\n",
    "sample_submission.to_csv('../submission.csv', index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Print the AUC Score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc = sum(best_aucs) / len(best_aucs)\n",
    "cv_auc"
   ]
  }
 ]
}